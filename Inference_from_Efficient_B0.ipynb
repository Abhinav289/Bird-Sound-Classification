{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 91844,
          "databundleVersionId": 11361821,
          "sourceType": "competition"
        },
        {
          "sourceId": 11060723,
          "sourceType": "datasetVersion",
          "datasetId": 6891568
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "3el4vI8XJ3BV"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "birdclef_2025_path = kagglehub.competition_download('birdclef-2025')\n",
        "kadircandrisolu_birdclef25_effnetb0_starter_weight_path = kagglehub.dataset_download('kadircandrisolu/birdclef25-effnetb0-starter-weight')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ef4ssFj2J3BZ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BirdCLEF 2025 Inference Notebook**\n",
        "This notebook runs inference on BirdCLEF 2025 test soundscapes and generates a submission file. It supports both single model inference and ensemble inference with multiple models. You can find the pre-processing and training processes in the following notebooks:\n",
        "\n",
        "- [Transforming Audio-to-Mel Spec. | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/transforming-audio-to-mel-spec-birdclef-25)  \n",
        "- [EfficientNet B0 Pytorch [Train] | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25)\n",
        "\n",
        "**Features**\n",
        "- Audio Preprocessing\n",
        "- Test-Time Augmentation (TTA)"
      ],
      "metadata": {
        "id": "On2tjqyNJ3BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "import logging\n",
        "import time\n",
        "import math\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T06:44:41.353955Z",
          "iopub.execute_input": "2025-04-01T06:44:41.354318Z",
          "iopub.status.idle": "2025-04-01T06:44:54.493009Z",
          "shell.execute_reply.started": "2025-04-01T06:44:41.354286Z",
          "shell.execute_reply": "2025-04-01T06:44:54.491985Z"
        },
        "id": "DkOwcME3J3Bb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "\n",
        "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
        "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
        "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
        "    model_path = '/kaggle/input/birdclef25-effnetb0-starter-weight'\n",
        "\n",
        "    # Audio parameters\n",
        "    FS = 32000\n",
        "    WINDOW_SIZE = 5\n",
        "\n",
        "    # Mel spectrogram parameters\n",
        "    N_FFT = 1024\n",
        "    HOP_LENGTH = 512\n",
        "    N_MELS = 128\n",
        "    FMIN = 50\n",
        "    FMAX = 14000\n",
        "    TARGET_SHAPE = (256, 256)\n",
        "\n",
        "    model_name = 'efficientnet_b0'\n",
        "    in_channels = 1\n",
        "    device = 'cpu'\n",
        "\n",
        "    # Inference parameters\n",
        "    batch_size = 16\n",
        "    use_tta = False\n",
        "    tta_count = 3\n",
        "    threshold = 0.5\n",
        "\n",
        "    use_specific_folds = False  # If False, use all found models\n",
        "    folds = [0, 1]  # Used only if use_specific_folds is True\n",
        "\n",
        "    debug = False\n",
        "    debug_count = 3\n",
        "\n",
        "cfg = CFG()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T06:45:11.64599Z",
          "iopub.execute_input": "2025-04-01T06:45:11.64638Z",
          "iopub.status.idle": "2025-04-01T06:45:11.652782Z",
          "shell.execute_reply.started": "2025-04-01T06:45:11.646348Z",
          "shell.execute_reply": "2025-04-01T06:45:11.651559Z"
        },
        "id": "-UPyjtt1J3Bc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using device: {cfg.device}\")\n",
        "print(f\"Loading taxonomy data...\")\n",
        "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
        "species_ids = taxonomy_df['primary_label'].tolist()\n",
        "num_classes = len(species_ids)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T06:45:20.580241Z",
          "iopub.execute_input": "2025-04-01T06:45:20.580604Z",
          "iopub.status.idle": "2025-04-01T06:45:20.60551Z",
          "shell.execute_reply.started": "2025-04-01T06:45:20.580574Z",
          "shell.execute_reply": "2025-04-01T06:45:20.604467Z"
        },
        "id": "edSOKmkjJ3Bc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class BirdCLEFModel(nn.Module):\n",
        "    def __init__(self, cfg, num_classes):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.backbone = timm.create_model(\n",
        "            cfg.model_name,\n",
        "            pretrained=False,\n",
        "            in_chans=cfg.in_channels,\n",
        "            drop_rate=0.0,\n",
        "            drop_path_rate=0.0\n",
        "        )\n",
        "\n",
        "        if 'efficientnet' in cfg.model_name:\n",
        "            backbone_out = self.backbone.classifier.in_features\n",
        "            self.backbone.classifier = nn.Identity()\n",
        "        elif 'resnet' in cfg.model_name:\n",
        "            backbone_out = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "        else:\n",
        "            backbone_out = self.backbone.get_classifier().in_features\n",
        "            self.backbone.reset_classifier(0, '')\n",
        "\n",
        "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.feat_dim = backbone_out\n",
        "        self.classifier = nn.Linear(backbone_out, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        if isinstance(features, dict):\n",
        "            features = features['features']\n",
        "\n",
        "        if len(features.shape) == 4:\n",
        "            features = self.pooling(features)\n",
        "            features = features.view(features.size(0), -1)\n",
        "\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T06:45:25.237576Z",
          "iopub.execute_input": "2025-04-01T06:45:25.237914Z",
          "iopub.status.idle": "2025-04-01T06:45:25.245973Z",
          "shell.execute_reply.started": "2025-04-01T06:45:25.237888Z",
          "shell.execute_reply": "2025-04-01T06:45:25.244634Z"
        },
        "id": "cB1c_9QQJ3Bc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def audio2melspec(audio_data, cfg):\n",
        "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
        "    if np.isnan(audio_data).any():\n",
        "        mean_signal = np.nanmean(audio_data)\n",
        "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
        "\n",
        "    mel_spec = librosa.feature.melspectrogram(\n",
        "        y=audio_data,\n",
        "        sr=cfg.FS,\n",
        "        n_fft=cfg.N_FFT,\n",
        "        hop_length=cfg.HOP_LENGTH,\n",
        "        n_mels=cfg.N_MELS,\n",
        "        fmin=cfg.FMIN,\n",
        "        fmax=cfg.FMAX,\n",
        "        power=2.0\n",
        "    )\n",
        "\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
        "\n",
        "    return mel_spec_norm\n",
        "\n",
        "def process_audio_segment(audio_data, cfg):\n",
        "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
        "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
        "        audio_data = np.pad(audio_data,\n",
        "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)),\n",
        "                          mode='constant')\n",
        "\n",
        "    mel_spec = audio2melspec(audio_data, cfg)\n",
        "\n",
        "    # Resize if needed\n",
        "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
        "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    return mel_spec.astype(np.float32)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T06:45:29.230754Z",
          "iopub.execute_input": "2025-04-01T06:45:29.23117Z",
          "iopub.status.idle": "2025-04-01T06:45:29.238617Z",
          "shell.execute_reply.started": "2025-04-01T06:45:29.231112Z",
          "shell.execute_reply": "2025-04-01T06:45:29.237456Z"
        },
        "id": "NlfQnIwVJ3Bd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def find_model_files(cfg):\n",
        "    \"\"\"\n",
        "    Find all .pth model files in the specified model directory\n",
        "    \"\"\"\n",
        "    model_files = []\n",
        "\n",
        "    model_dir = Path(cfg.model_path)\n",
        "\n",
        "    for path in model_dir.glob('**/*.pth'):\n",
        "        model_files.append(str(path))\n",
        "\n",
        "    return model_files\n",
        "\n",
        "def load_models(cfg, num_classes):\n",
        "    \"\"\"\n",
        "    Load all found model files and prepare them for ensemble\n",
        "    \"\"\"\n",
        "    models = []\n",
        "\n",
        "    model_files = find_model_files(cfg)\n",
        "\n",
        "    if not model_files:\n",
        "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
        "        return models\n",
        "\n",
        "    print(f\"Found a total of {len(model_files)} model files.\")\n",
        "\n",
        "    if cfg.use_specific_folds:\n",
        "        filtered_files = []\n",
        "        for fold in cfg.folds:\n",
        "            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
        "            filtered_files.extend(fold_files)\n",
        "        model_files = filtered_files\n",
        "        print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
        "\n",
        "    for model_path in model_files:\n",
        "        try:\n",
        "            print(f\"Loading model: {model_path}\")\n",
        "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device))\n",
        "\n",
        "            model = BirdCLEFModel(cfg, num_classes)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model = model.to(cfg.device)\n",
        "            model.eval()\n",
        "\n",
        "            models.append(model)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {model_path}: {e}\")\n",
        "\n",
        "    return models\n",
        "\n",
        "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
        "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
        "    predictions = []\n",
        "    row_ids = []\n",
        "    soundscape_id = Path(audio_path).stem\n",
        "\n",
        "    try:\n",
        "        print(f\"Processing {soundscape_id}\")\n",
        "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
        "\n",
        "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
        "\n",
        "        for segment_idx in range(total_segments):\n",
        "            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
        "            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
        "            segment_audio = audio_data[start_sample:end_sample]\n",
        "\n",
        "            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
        "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
        "            row_ids.append(row_id)\n",
        "\n",
        "            if cfg.use_tta:\n",
        "                all_preds = []\n",
        "\n",
        "                for tta_idx in range(cfg.tta_count):\n",
        "                    mel_spec = process_audio_segment(segment_audio, cfg)\n",
        "                    mel_spec = apply_tta(mel_spec, tta_idx)\n",
        "\n",
        "                    mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                    mel_spec = mel_spec.to(cfg.device)\n",
        "\n",
        "                    if len(models) == 1:\n",
        "                        with torch.no_grad():\n",
        "                            outputs = models[0](mel_spec)\n",
        "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                            all_preds.append(probs)\n",
        "                    else:\n",
        "                        segment_preds = []\n",
        "                        for model in models:\n",
        "                            with torch.no_grad():\n",
        "                                outputs = model(mel_spec)\n",
        "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                                segment_preds.append(probs)\n",
        "\n",
        "                        avg_preds = np.mean(segment_preds, axis=0)\n",
        "                        all_preds.append(avg_preds)\n",
        "\n",
        "                final_preds = np.mean(all_preds, axis=0)\n",
        "            else:\n",
        "                mel_spec = process_audio_segment(segment_audio, cfg)\n",
        "\n",
        "                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                mel_spec = mel_spec.to(cfg.device)\n",
        "\n",
        "                if len(models) == 1:\n",
        "                    with torch.no_grad():\n",
        "                        outputs = models[0](mel_spec)\n",
        "                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                else:\n",
        "                    segment_preds = []\n",
        "                    for model in models:\n",
        "                        with torch.no_grad():\n",
        "                            outputs = model(mel_spec)\n",
        "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                            segment_preds.append(probs)\n",
        "\n",
        "                    final_preds = np.mean(segment_preds, axis=0)\n",
        "\n",
        "            predictions.append(final_preds)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "\n",
        "    return row_ids, predictions"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T06:45:33.958135Z",
          "iopub.execute_input": "2025-04-01T06:45:33.958531Z",
          "iopub.status.idle": "2025-04-01T06:45:33.975311Z",
          "shell.execute_reply.started": "2025-04-01T06:45:33.958493Z",
          "shell.execute_reply": "2025-04-01T06:45:33.974213Z"
        },
        "id": "wc1mWfiDJ3Bd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_tta(spec, tta_idx):\n",
        "    \"\"\"Apply test-time augmentation\"\"\"\n",
        "    if tta_idx == 0:\n",
        "        # Original spectrogram\n",
        "        return spec\n",
        "    elif tta_idx == 1:\n",
        "        # Time shift (horizontal flip)\n",
        "        return np.flip(spec, axis=1)\n",
        "    elif tta_idx == 2:\n",
        "        # Frequency shift (vertical flip)\n",
        "        return np.flip(spec, axis=0)\n",
        "    else:\n",
        "        return spec\n",
        "\n",
        "def run_inference(cfg, models, species_ids):\n",
        "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
        "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
        "\n",
        "    if cfg.debug:\n",
        "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
        "        test_files = test_files[:cfg.debug_count]\n",
        "\n",
        "    print(f\"Found {len(test_files)} test soundscapes\")\n",
        "\n",
        "    all_row_ids = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for audio_path in tqdm(test_files):\n",
        "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
        "        all_row_ids.extend(row_ids)\n",
        "        all_predictions.extend(predictions)\n",
        "\n",
        "    return all_row_ids, all_predictions\n",
        "\n",
        "def create_submission(row_ids, predictions, species_ids, cfg):\n",
        "    \"\"\"Create submission dataframe\"\"\"\n",
        "    print(\"Creating submission dataframe...\")\n",
        "\n",
        "    submission_dict = {'row_id': row_ids}\n",
        "\n",
        "    for i, species in enumerate(species_ids):\n",
        "        submission_dict[species] = [pred[i] for pred in predictions]\n",
        "\n",
        "    submission_df = pd.DataFrame(submission_dict)\n",
        "\n",
        "    submission_df.set_index('row_id', inplace=True)\n",
        "\n",
        "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
        "\n",
        "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
        "    if missing_cols:\n",
        "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
        "        for col in missing_cols:\n",
        "            submission_df[col] = 0.0\n",
        "\n",
        "    submission_df = submission_df[sample_sub.columns]\n",
        "\n",
        "    submission_df = submission_df.reset_index()\n",
        "\n",
        "    return submission_df\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T06:45:44.029229Z",
          "iopub.execute_input": "2025-04-01T06:45:44.029611Z",
          "iopub.status.idle": "2025-04-01T06:45:44.039684Z",
          "shell.execute_reply.started": "2025-04-01T06:45:44.029568Z",
          "shell.execute_reply": "2025-04-01T06:45:44.038299Z"
        },
        "id": "eGDnrHiRJ3Be"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    start_time = time.time()\n",
        "    print(\"Starting BirdCLEF-2025 inference...\")\n",
        "    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
        "\n",
        "    models = load_models(cfg, num_classes)\n",
        "\n",
        "    if not models:\n",
        "        print(\"No models found! Please check model paths.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
        "\n",
        "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
        "\n",
        "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
        "\n",
        "    submission_path = 'submission.csv'\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Submission saved to {submission_path}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T06:45:49.49287Z",
          "iopub.execute_input": "2025-04-01T06:45:49.493217Z",
          "iopub.status.idle": "2025-04-01T06:45:49.49952Z",
          "shell.execute_reply.started": "2025-04-01T06:45:49.493193Z",
          "shell.execute_reply": "2025-04-01T06:45:49.498196Z"
        },
        "id": "6rTIdeIXJ3Be"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T06:45:52.516968Z",
          "iopub.execute_input": "2025-04-01T06:45:52.517446Z",
          "iopub.status.idle": "2025-04-01T06:45:54.702005Z",
          "shell.execute_reply.started": "2025-04-01T06:45:52.517402Z",
          "shell.execute_reply": "2025-04-01T06:45:54.700837Z"
        },
        "id": "FsML0_yxJ3Be"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "-PyduGUBJ3Be"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}